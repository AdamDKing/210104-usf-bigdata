MapReduce Whitepaper:
- What does the Map part of MapReduce take in?  What does it output? (1)
- What about the Reduce part of MapReduce? (1)
- What is the optional Combiner step? (1)
- What is GFS? (2)
- How many replications of data does GFS provide by default?  Block size? (2)
  - HDFS, Hadoop Distributed FileSystem, is the same by default (v1 is the same, v2+ is 128MB)
- Why is the Combiner step useful in the case of a wordcount? (2)
- Is the Master fault tolerant?  Why or why not? (3)
- Are the workers fault tolerant?  Why or why not? (3)
- What happens to the output of completed Map tasks on failed worker nodes? (3)
- What is Data Locality?  Why is it important? (4)
- How does MapReduce increase processing speed when some machines in the cluster are running slowly but don't fail? (4)
- What does a custom partitioning function change about how your MapReduce job runs? (4)
- What is a hashcode?  How are hashcodes used in (default) partitions? (5)
- What does it mean that side effects should be atomic and idempotent? (5)
- What are Counters for? (5)
- Where do Map tasks read from?  Write to? (6)
- Where do Reduce tasks read from?  Write to? (6)
- Which of the above I/O steps would you expect to take the longest? (6)
- Walk thorugh a MapReduce job that counts words in the sentence : "the quick brown fox jumped over the lazy dog" (6)
  - How does this work for 2 input blocks (so 2 Map tasks) and 1 reduce task?  What if we had 2 Reduce tasks?